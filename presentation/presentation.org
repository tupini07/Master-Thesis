#+STARTUP: beamer
#+title: Soweego: Ensemble learning applied to Record Linkage

#+BEAMER_HEADER: \titlegraphic{\includegraphics[width=.4\textwidth]{../graphics/logo_unitn.png}}

#+author: Andrea Tupini
#+EMAIL:  andrea.tupini@studenti.unitn.it 
#+DATE:   2019-10-23

#+latex_header: \usepackage{multicol}
#+options: H:2 toc:nil num:t
#+latex_class: beamer
#+LATEX_CLASS_OPTIONS: [presentation]
#+columns: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+beamer_theme: Dresden
#+beamer_color_theme: orchid
#+beamer_font_theme:
#+beamer_inner_theme:
#+beamer_outer_theme:
#+beamer_header:
#+latex_header: \AtBeginSection[] {\begin{frame}{Outline} \begin{multicols}{2} \tableofcontents[currentsection,hideallsubsections,sectionstyle=show/shaded] \end{multicols} \end{frame}} }
 

* Problem
** Wikidata
   
   - Powers Wikimedia Foundation projects
   - Collection of *items*, each of which is associated with some *statements*.

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:
    
    Good morning, my name is Andrea Tupini. The title of my dissertation is
    /Soweego: ensemble learning applied to record linkage/.

    This project was made for Wikidata. But what is Wikidata?

    Wikidata is one of the biggest knowledge bases in the web. It powers the
    Wikipedia free encyclopedia, as well as all the of the other Wikimedia
    foundation projects.
    
    Wikidata contains information, or claims, about different /items/. In
    Wikidata lingo, these claims are called /statements/. 

    An example of such statement can be /when or where a person was born/.

** Data quality and trust
   
*** Quality
    Identifiers enable feedback loop

*** Trust
    References to external reliable sources

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    Wikidata strives to have the highest possible data quality and
    trustworthiness. 

    An important part of this is having references from Wikidata to trusted
    external sources.

    If a claim about an item is backed up by an external reference then users
    can go and check this reference and corroborate that the information stated
    in Wikidata is actually correct.

    If, on the other hand, the information is in any way incorrect then the
    statements in Wikidata can be flagged or directly fixed by the community.

  
** Example


*** Left                                                           :B_column:
    :PROPERTIES:
    :BEAMER_env: column
    :BEAMER_col: 0.5
    :END:

    #    #+ATTR_LATEX: :width 0.3\textwidth
    #    [[../graphics/Wikidata-logo-en.png]]

    #+caption: Wikidata (Q42)
    #+ATTR_LATEX: :width 0.6\textwidth
    [[../graphics/douglas_adams_headshot_wikidata.jpg]]


*** Right                                                          :B_column:
    :PROPERTIES:
    :BEAMER_env: column
    :BEAMER_col: 0.5
    :END:

    #    #+ATTR_LATEX: :width 0.3\textwidth
    #    [[../graphics/imdb_logo.png]]

    #+caption: IMDb (nm0010930)
    #+ATTR_LATEX: :width 0.6\textwidth
    [[../graphics/douglas_adams_headshot_imdb.png]]

    
*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:
    
    For example, we have the items, which are also called entities, representing
    the writer /Douglas Adams/, who is the writer of the book /hitchikers guide
    to the galaxy/. Say we go to Wikidata to check when he was born. In Wikidata
    we see that there is a link to the entity in another dataset, which is also
    called a catalog, namely in /IMDb/. If for some reason we have doubts about
    the claims made in Wikidata we can then go ahead and check if in IMDb the
    same claims are made.

** Missing References                                              
    
*** Left
    :PROPERTIES:
    :BEAMER_env: column
    :BEAMER_col: 0.5
    :END:

   There are a total of /744.4 million/ *statements*
    
   - /200.5 million/ are unreferenced (26%)

*** Right                                                          :B_column:
    :PROPERTIES:
    :BEAMER_env: column
    :BEAMER_col: 0.5
    :END:

   #+ATTR_LATEX: :width \textwidth
   [[../graphics/pie_wikidata_referenced_unreferenced.png]]

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    Even though having references is very important for data quality and
    trustworthiness, around 26% of all statements in Wikidata are missing
    references.

    This means that users have no way of verifying if the information claimed by
    these statements is actually correct, since there is no trusted, external
    source of data they can go and check.

    
* Soweego Introduction

  
** What is soweego?

   - Open source project of the /Wikimedia Foundation/
   - Uses record linkage and supervised machine learning to link Wikidata entities with external /catalogs/

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    Soweego was conceived with the goal of improving the existing situation in
    Wikidata by automatically adding external references. Soweego will take
    wikidata entities, which are the different items in wikidata, and link them
    with entities in other databases.

    For example, soweego would link /douglas adams/ with the respective /douglas
    adams/ in imdb. 

    More formally, Soweego is an open source project for the Wikimedia
    Foundation. It uses the technique of record linkage to find matching entity
    pairs between Wikidata and other external dataset. And supervised machine
    learning classifiers to tell it when a pair of entities is actually a match
    or not.

    We say that Wikidata is our /source catalog/, the source of the entities
    we're interested in linking. And the external dataset is said to be our
    /target catalog/, that is, the dataset we want to link Wikidata to.

** Overview of soweego

    - Input: pairs of /entities/
      - source = Wikidata
      - target = target catalog 
    - Output: *links* as Wikidata identifier statements
    # - Output: *probability* that each pair represents the same entity 

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    The way in which soweego works is that it gets as input a target catalog we
    want to link Wikidata with. And we then get as an output, statements in
    wikidata linking each entity with the respective target entity.

** Example

    #+ATTR_LATEX: :width \textwidth
    [[../graphics/douglas_adams_Wikidata_head.png]]


    #+ATTR_LATEX: :width \textwidth
    [[../graphics/douglas_adams_Wikidata_imdb_identifier.png]]

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    For example, in the case of /Douglas Adamas/, here we have his page on
    wikidata. At the top we can see his ID in Wikidata, and the information
    below tells us which is his ID in IMDb. In this way, Wikidata entities can
    be linked with any number of external catalogs.

** External Catalogs

*** Used catalogs                                                   :B_frame:
    :PROPERTIES:
    :BEAMER_env: frame
    :END:

    As a starting point, /soweego/ focuses on the domain of *people*, which
    according to Wikidata Statistics represent around 10% of all entities.
    
    \hfill
    
    Targets used:

    - IMDb
    - Musicbrainz
    - Discogs

**** Notes                                                           :B_note:
     :PROPERTIES:
     :BEAMER_env: note
     :END:

     In principle soweego can be used to link Wikidata with any catalogs, of any
     kinds of entities. But for the purpose of the project, the scope was
     constrained to only work with people. 

     This was manly because people, according to the wikipedia statistics page,
     represent around 10% all of the entities in Wikidata. Which is quite a big
     chunk.

     Specifically, soweego starts by linking wikidata entities with the target catalogs
     of imdb, musicbrainz, and discogs.

*** IMDb                                                            :B_frame:
    :PROPERTIES:
    :BEAMER_env: frame
    :END:
    
    | Entity Type | Training | Classification |
    |-------------+----------+----------------|
    | Actor       | 177,417  | 89,081         |
    | Director    | 46,904   | 8,407          |
    | Musician    | 61,034   | 210,017        |
    | Producer    | 19,861   | 2,075          |
    | Writer      | 41,373   | 15,143         |

**** Notes                                                           :B_note:
     :PROPERTIES:
     :BEAMER_env: note
     :END:
     
     So, IMDb is the internet movie database. From it we get information about
     entities which are actors, directors, musicians, producers, and writers.

     In the table we see how many training and classification examples we have
     available for each of these IMDb entity types.

*** Musicbrainz                                                     :B_frame:
    :PROPERTIES:
    :BEAMER_env: frame
    :END:

    | Entity Type | Training | Classification |
    |-------------+----------+----------------|
    | Band        | 47,096   | 32,407         |
    | Musician    | 118,107  | 153,437        |

**** Notes                                                           :B_note:
     :PROPERTIES:
     :BEAMER_env: note
     :END:

     Musicbrainz is a catalog which contains information about musicians and
     bands, and the relation among them. 

     Here we can also see how many training and classification samples we have
     for each entity type.

*** Discogs                                                         :B_frame:
    :PROPERTIES:
    :BEAMER_env: frame
    :END:

    | Entity Type | Training | Classification |
    |-------------+----------+----------------|
    | Band        | 43,428   | 36,200         |
    | Musician    | 88,069   | 183,483        |

**** Notes                                                           :B_note:
     :PROPERTIES:
     :BEAMER_env: note
     :END:

     Finally, Discogs is also a catalog which has information about musicians,
     bands, and the relation among them.

* Soweego Pipeline

** Pipeline steps

   1) Importer
   2) Blocking
   3) Data preprocessing
   4) Feature extraction
   5) Linker
   6) Upload results

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:
    
    The inner workings of soweego can be represented as a pipeline of six steps.
    Namely, these are: importer, blocking, data preprocessing, feature
    extraction, linker, and uploading the results to Wikidata.

** Importer
   
   - Download data from catalogs
   - Transform it into a common structure
   - Save it into soweego's internal database

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    The importer is in charge of downloading the dataset from the external
    catalog, transforms it into a standardized representation, and finally it
    saves the data to soweego's internal database.

** Blocking

   - Reduces complexity
   - Only compare pairs of entities which have a similar name

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    Blocking allows us to reduce the complexity of the process since if we were
    to compare an entity in Wikidata against every entity in IMDb, for example,
    then we would need to do N comparisons, where N is the number of entities in
    IMDb.

    Blocking allows us to compare only a subset of all the entities by using a
    blocking rule to choose which entities in the target catalog may be a
    potential match. In soweego, we compare against the wikidata entity, only
    those target entities which share a part of the name. For example, /douglas
    adams/ would be compared with all those entities in IMDb which have either
    /douglas/ or /adams/ as part of their name.

    This greatly reduces the number of comparisons that need to be made.

    The output of blocking is a list of all pairs which might be a potential
    match according to the blocking rule.

** Data preprocessing
   
   - Normalize: strip \rightarrow ASCII \rightarrow lowercase
   - Tokenize: split \rightarrow no 1-character \rightarrow stopwords
   - Handle dates: parse \rightarrow pair precision
   - Clean datasets

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:
    
    In data preprocessing, we normalize and tokenize text, we standardize dates
    to a common format, and we drop any entities, or attributes of those
    entities, which are empty. 

** Feature extraction

*** Definition                                                      :B_frame:
    :PROPERTIES:
    :BEAMER_env: frame
    :END:

   - Field pair comparison
   - Fields \rightarrow Similarity Function \rightarrow Score
   - One feature vector which characterizes similarity between pair

**** Notes                                                           :B_note:
     :PROPERTIES:
     :BEAMER_env: note
     :END:

     In feature extraction we get as input the preprocessed pairs of potential
     matches from the previous step. For each of these we extract a feature
     vector, where each value in the feature vector is the result of applying a
     comparison function on a pair of attributes.

     Comparison functions tell us, as a percentage, how similar a pair of fields
     is. Where zero means the attributes are completely different, and one means
     they are exactly the same.

*** Comparison functions                                            :B_frame:
    :PROPERTIES:
    :BEAMER_env: frame
    :END:

    
**** Left                                                          :B_column:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :BEAMER_env: column
     :END:
     
     - Exact match
     - Similar strings
       - Levenshtein
       - Cosine

**** Right                                                         :B_column:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :BEAMER_env: column
     :END:

     - Similar dates
     - Shared tokens
     - Shared occupations

**** Notes                                                           :B_note:
     :PROPERTIES:
     :BEAMER_env: note
     :END:

     For soweego, we use the following comparison functions to check the
     similarity among attributes. 

     We check if fields are an exact match, we also check similar strings and
     dates, and the percentage of shared text tokens and occupations.

** Linking

   - Takes feature vectors and returns probability that pair is a match

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    The next step is linking. Here a supervised machine learning algorithm takes
    the feature vectors extracted in the previous step and tells the probability
    that this feature vector corresponds to a pair of entities which is a match.

    The focus of this dissertation is on implementing the linking procedure.

** Upload results
   
   Define /upper/ t_U and /lower/ t_L thresholds.
 
   - *non-match*, if /prob \leq t_L/
   - *potential-match*, if /t_L < prob < t_U/
   - *match*, if /t_U \leq prob/

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    For this step, we get the probability that a pair is a match. And depending
    these thresholds the pair is classified as a match, non-match, or potential
    match.

    In the case of a match, the appropriate statement is created on the wikidata
    entity. 

    If it is a non-match, then the prediction is simply dropped. 

    If it is a potential match then the pair is uploaded to a service called
    Mix'n'Match which is a service where volunteers can manually review the
    pairs and manually mark them as matches or non-matches.

* Algorithms

** Baseline classifiers

   - Linear SVM
   - Naive Bayes
   - Logistic Regression
   - Random Forest
   - Single-Layer Perceptron
   - Multi-Layer Perceptron

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    These are the algorithms that soweego was originally conceived to use. 

    However, after implementing and evaluating them, we asked ourselves if there
    was a way to improve the performance by mixing the opinions of some of them.

    This is why we considered using ensemble models. In ensemble models, we use a pool
    of base classifiers and mix the predictions of these base classifiers. 

    For our case, all the baseline classifiers, except for linear support vector
    machines, were used as the pool of base classifiers.

** Ensemble classifiers

   - Soft Voting Classifier
   - Hard Voting Classifier
   - Gated Ensemble
   - Stacked Ensemble
   
*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    These are the ensemble models implemented for soweego. 

    The soft voting classifier basically takes the average of the predictions
    given by all base classifiers. 

    In hard voting classifier, the final prediction is obtained by having each
    base classifier voting for either match or non match. The label which had
    the most votes is the one predicted by the ensemble.

    Gated ensemble and stacked ensembles are stack based ensembles. This
    technique basically allows us to stack classifiers in layers, in which the
    next layer will get as input the predictions given by the previous layer of
    classifiers.

    The last layer in this kind of ensembles is called the meta layer, and is
    another classifier in charge of learning how to join the predictions it
    receives from lower layers.

    The gated ensemble is basically composed of the baseline classifiers, and
    then a single layer perceptron as the meta layer. The idea for this is for
    the single layer perceptron to find the optimal weight to assign to each
    base classifier. 

    A stacked based ensemble is very similar. The difference is that now we have
    two layers of base classifiers, followed by a single layer perceptron as a
    meta layer.

* Results

** How are results presented
   
   The results will be presented in terms of the following metrics:

   - Precision
   - Recall
   - F1 Score

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    We present the results in terms of precision, recall, and F1 score.

    The results presented here were obtained by evaluating the performance of
    each classifier on all entity types and then averaging the performances. So
    they were evaluated for linking Wikidata with IMDb/Actors,
    Discogs/Musicians, etc..



** Baseline results

   | Model                   | Precision |   Recall |       F1 |
   |-------------------------+-----------+----------+----------|
   | Multi-Layer Perceptron  |  *0.9166* |   0.9349 | *0.9349* |
   |-------------------------+-----------+----------+----------|
   | Random Forest           |    0.9145 |   0.9307 |   0.9223 |
   |-------------------------+-----------+----------+----------|
   | Logistic Regression     |    0.9121 |   0.9314 |   0.9215 |
   |-------------------------+-----------+----------+----------|
   | Single-Layer Perceptron |    0.9145 |   0.9284 |   0.9212 |
   |-------------------------+-----------+----------+----------|
   | Linear SVM              |    0.9093 |   0.9342 |   0.9210 |
   |-------------------------+-----------+----------+----------|
   | Naive Bayes             |    0.8863 | *0.9490* |   0.9151 |

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    For the baseline results, we see that multi layer perceptron is the one that
    has the best average precision and F1 score, and it is also the second best
    with respect to recall. The best recall score is obtained by the naive bayes
    classifier, although this also has the worst precision and F1 scores.

    Baseline classifiers give quite good results, but there is still some room
    for improvement, especially when it comes to the precision.

** Ensemble results

   | Model            | Precision |   Recall |       F1 |
   |------------------+-----------+----------+----------|
   | Soft Voting      |    0.9199 | *0.9308* | *0.9248* |
   |------------------+-----------+----------+----------|
   | Gated Ensemble   |    0.9227 |   0.9268 |   0.9245 |
   |------------------+-----------+----------+----------|
   | Hard Voting      |    0.9145 |   0.9344 |   0.9239 |
   |------------------+-----------+----------+----------|
   | Stacked ensemble |  *0.9235* |   0.9242 |   0.9234 |

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    For ensemble models we see that they tend to have a higher precision and
    lower recall than the baseline classifiers. 

    Stacked ensemble is the one that gate the highest precision. While soft
    voting ensemble is the one with the highest recall and F1 score. 

** 5 best classifiers

   | Model                  | Precision    | Recall       | F1           |
   |------------------------+--------------+--------------+--------------|
   | Multi-Layer Perceptron | 0.9166 (4)   | *0.9349* (1) | *0.9255* (1) |
   |------------------------+--------------+--------------+--------------|
   | Soft Voting            | 0.9199 (3)   | 0.9308 (3)   | 0.9248 (2)   |
   |------------------------+--------------+--------------+--------------|
   | Gated Ensemble         | 0.9227 (2)   | 0.9268 (4)   | 0.9245 (3)   |
   |------------------------+--------------+--------------+--------------|
   | Hard Voting            | 0.9145 (5)   | 0.9344 (2)   | 0.9239 (4)   |
   |------------------------+--------------+--------------+--------------|
   | Stacked Ensemble       | *0.9235* (1) | 0.9242 (5)   | 0.9234 (5)   |

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:
    
    If we consider the top five classifiers among all of these, ensembles and
    baseline, we get this table. It is ordered by F1 score, in decreasing order,
    and the number to the right of each score states the relative position of
    the classifier for that metric. 

    So, we can see that multi layer perceptron performs pretty well. It has the
    highest recall and F1 score. But is the second worst when it comes to
    precision. 

    Stacked ensemble is, among all, the one which has the best average
    precision. 

    Also note that all of the four ensemble classifiers used appear among the
    top 5.


* Discussion

** Soweego usage 
   
   - We don't want to degrade the quality of the data
   - Unsure matches can be uploaded to Mix'n'Match

*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    Even though multi layer perceptron has the highest F1 score, in the case of
    soweego we want to upload to wikidata only those links which we are sure
    about, since we want to impact the data quality as least as possible. This
    means that we prefer to use a classifier which has a higher precision.

    Links of which the classifier predicts as potential matches, can still be
    uploaded to mix'n'match for manual classification, so that none of the links
    are lost. 

** Iterative training

   - Repeated executions
   - Next execution gets output of previous one
     
*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    Also note that we've talked until now about a single execution of soweego.
    In reality, soweego is meant to be executed repeatedly after a certain
    period of time. For example, it will be executed once a month. 

    In the next iteration, soweego will get as part of its training data,
    predictions which it uploaded in the past iteration. Meaning that every time
    the training set will be larger. 

    This is another reason to prefer a high precision classifier, since
    uploading incorrect links to Wikidata means that the training set in the
    next iteration may contain more of these incorrect examples.

    Also, during these intervals, the community will review the links uploaded
    by soweego and clean out some of the ones that are not correct.

* Conclusions


** Conclusions

  - Link Wikidata to external catalogs
  - Record linkage as supervised machine learning
  - Favor high precision to avoid degrading data quality
  - Effective /in vitro/ performance
    - /In situ/ evaluation is happening right now
    
*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:
    
    As we've seen, Soweego links wikidata to external catalogs by using record
    linkage and supervised machine learning classifiers. 

    Using an ensemble algorithm or other high precision classifier, ensures that
    the information uploaded to wikidata is as trustworthy as possible.
  
    Ensemble classifiers, at least with respect to the results obtained in this
    project, seem to actually improve the performance, and specifically the
    precision. 
    
    The evaluation metrics presented here are only on the training set. However,
    the training set is not really a correct representation of the
    classification set. Evaluation of the final predictions, made on the
    classification set, is currently being done by the community.


** Future Work

  - Better construction of base classifiers
  - Leverage relations among entities
    
*** Notes                                                            :B_note:
    :PROPERTIES:
    :BEAMER_env: note
    :END:

    As always there is still some room for improvement. 

    For instance, we can do a better choice of which are the base classifiers for the
    ensembles instead of using all the baseline classifiers.

    In our data we also have some information about the relation among entities.
    For instance, which musicians are part of which bands, or which actors
    appeared in which movies. This relation information could be leveraged to
    make better predictions. 
